from __future__ import annotations\nfrom typing import List, Dict, Iterable, Optional\nimport json\nimport logging\n\ntry:\n    from psycopg2.extras import execute_values\nexcept Exception:  # pragma: no cover - allows import in environments without psycopg2\n    execute_values = None  # type: ignore\n\nlogger = logging.getLogger(__name__)\n\n\nclass DataLoader:\n    \"\"\"\n    DataLoader handles bulk inserts and upserts into JSONB staging tables following the\n    common schema (data JSONB, source_id, etl_job_id, etl_run_id, timestamps).\n\n    Methods:\n      - load_to_staging: convenience that performs an upsert by source_id\n      - upsert_records: upsert on a given key field (defaults to source_id)\n      - bulk_insert: straight bulk insert (no conflict handling)\n      - truncate_and_reload: truncate then insert\n    \"\"\"\n\n    def __init__(self, conn, batch_size: int = 1000):\n        self.conn = conn\n        self.batch_size = batch_size\n\n    def load_to_staging(self, table_name: str, records: List[Dict], job_id: int, run_id: int, key_field: str = "source_id") -> int:\n        if not records:\n            logger.info("No records to load for table %s", table_name)\n            return 0\n        # Derive source_id from record if present, else None\n        prepared = [\n            (json.dumps(r), r.get(key_field), job_id, run_id) for r in records\n        ]\n        return self._upsert_jsonb(table_name, prepared, key_field=key_field)\n\n    def upsert_records(self, table_name: str, records: List[Dict], key_field: str = "source_id") -> int:\n        if not records:\n            return 0\n        prepared = [\n            (json.dumps(r), r.get(key_field)) for r in records\n        ]\n        return self._upsert_jsonb(table_name, prepared, key_field=key_field, include_lineage=False)\n\n    def bulk_insert(self, table_name: str, records: List[Dict], job_id: Optional[int] = None, run_id: Optional[int] = None) -> int:\n        if not records:\n            return 0\n        prepared = [\n            (json.dumps(r), r.get("source_id"), job_id, run_id) for r in records\n        ]\n        sql = f"""\n            INSERT INTO {table_name} (data, source_id, etl_job_id, etl_run_id)\n            VALUES %s\n        """\n        return self._execute_values(sql, prepared)\n\n    def truncate_and_reload(self, table_name: str, records: List[Dict], job_id: Optional[int] = None, run_id: Optional[int] = None) -> int:\n        with self.conn.cursor() as cur:\n            cur.execute(f"TRUNCATE TABLE {table_name}")\n        self.conn.commit()\n        return self.bulk_insert(table_name, records, job_id=job_id, run_id=run_id)\n\n    # Internal helpers\n    def _upsert_jsonb(self, table_name: str, prepared_values: List[tuple], key_field: str = "source_id", include_lineage: bool = True) -> int:\n        columns = "(data, {kf}, etl_job_id, etl_run_id)" if include_lineage else "(data, {kf})"\n        columns = columns.format(kf=key_field)\n        set_clause = "data = EXCLUDED.data, updated_at = NOW()"\n        sql = f"""\n            INSERT INTO {table_name} {columns}\n            VALUES %s\n            ON CONFLICT ({key_field}) DO UPDATE SET\n                {set_clause}\n        """\n        return self._execute_values(sql, prepared_values)\n\n    def _execute_values(self, sql: str, values: List[tuple]) -> int:\n        total = 0\n        with self.conn.cursor() as cur:\n            for i in range(0, len(values), self.batch_size):\n                batch = values[i : i + self.batch_size]\n                if execute_values is None:\n                    # Fallback for environments without psycopg2; useful in dry tests\n                    for v in batch:\n                        cur.execute(sql.replace("VALUES %s", "VALUES (%s, %s, %s, %s)"), v)  # type: ignore\n                        total += 1\n                else:\n                    execute_values(cur, sql, batch)\n                    total += len(batch)\n        self.conn.commit()\n        return total\n